{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Entities\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import collections\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "pl.seed_everything(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import FB15k_237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FB15k_237('data/FB15k_237', split='train')[0]\n",
    "val_data = FB15k_237('data/FB15k_237', split='val')[0]\n",
    "test_data = FB15k_237('data/FB15k_237', split='test')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node_features = torch.zeros((train_data.num_nodes, len(torch.unique(train_data.edge_type))), dtype=torch.float16)\n",
    "for i in range(len(train_data.edge_type)):\n",
    "    b = train_data.edge_type[i]\n",
    "    a = train_data.edge_index[0, i]\n",
    "    \n",
    "    train_node_features[a, b] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, triplets=None, in_dim=237, hidden_dim=256, num_relations = 237, dropout=0.3, num_hidden = 0, num_bases:int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.anchors, self.positives, self.negatives = triplets\n",
    "\n",
    "        num_anchors = len(self.anchors)\n",
    "        permutation = torch.randperm(num_anchors)\n",
    "\n",
    "        self.anchors = self.anchors[permutation]\n",
    "        self.positives = self.positives[permutation]\n",
    "        self.negatives = self.negatives[permutation]\n",
    "\n",
    "        num_train = int(0.9 * num_anchors)\n",
    "\n",
    "        self.train_anchors, self.train_pos, self.train_neg = self.anchors[:num_train], self.positives[:num_train], self.negatives[:num_train]\n",
    "        self.val_anchors, self.val_pos, self.val_neg = self.anchors[num_train:], self.positives[num_train:], self.negatives[num_train:]\n",
    "    \n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(pyg_nn.RGCNConv(in_dim, hidden_dim, num_relations, num_bases=num_bases))\n",
    "        for i in range(num_hidden):\n",
    "            self.layers.append(pyg_nn.RGCNConv(hidden_dim, hidden_dim, num_relations, num_bases=num_bases))\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.norm = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.loss = torch.nn.TripletMarginLoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i](x, edge_index, edge_type)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.norm(x)\n",
    "\n",
    "        x = self.layers[-1](x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        emb = self(batch.x.float(), batch.edge_index, batch.edge_type)\n",
    "        loss = self.loss(emb[self.train_anchors], emb[self.train_pos], emb[self.train_neg])\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        emb = self(batch.x.float(), batch.edge_index, batch.edge_type)\n",
    "        loss = self.cost(emb[self.val_anchors], emb[self.val_pos], emb[self.val_neg])\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/moritzduck/miniconda3/envs/graphml/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/moritzduck/miniconda3/envs/graphml/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Missing logger folder: /Users/moritzduck/workspaces/graphml/project2/lightning_logs\n",
      "/Users/moritzduck/miniconda3/envs/graphml/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "data_list = [torch_geometric.data.Data(x=train_node_features, edge_index=train_data.edge_index, edge_type=train_data.edge_type)]\n",
    "loader = torch_geometric.loader.DataLoader(data_list, shuffle=False, batch_size=1, num_workers=2)\n",
    "\n",
    "triplets = torch_geometric.utils.structured_negative_sampling(train_data.edge_index)\n",
    "\n",
    "model = ContrastiveModel(triplets=triplets, num_bases=42)\n",
    "trainer = pl.Trainer(max_epochs=100, enable_model_summary=False, enable_progress_bar=False, \\\n",
    "                    log_every_n_steps=1, accelerator='auto')\n",
    "\n",
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = data_list[0].x.float()\n",
    "edge_index = data_list[0].edge_index\n",
    "edge_type = data_list[0].edge_type\n",
    "embeddings = model(x, edge_index, edge_type).detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(embeddings, \"embeddings.pt\")\n",
    "#embeddings = torch.load(\"embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scoring(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,num_relations = 237,num_nodes = -1,hidden_dim = 256,lr = 1e-3,batch_size: int = 1):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.num_relations = num_relations\n",
    "        self.num_nodes = num_nodes\n",
    "       \n",
    "        self.Wr = torch.nn.Bilinear(hidden_dim, hidden_dim, 2, bias = False)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        target, source = batch\n",
    "        pos_labels = torch.zeros((target.shape[0],2), dtype=torch.float)\n",
    "        neg_labels = torch.zeros((target.shape[0],2), dtype=torch.float)\n",
    "        pos_labels[:,0] = 1\n",
    "        neg_labels[:,1] = 1\n",
    "        \n",
    "        \n",
    "        idx_target = torch.randint(0, self.num_nodes, (target.shape[0],))\n",
    "        idx_source = torch.randint(0, self.num_nodes, (target.shape[0],))\n",
    "        neg_samples_target = self.embeddings[idx_target]\n",
    "        neg_samples_source = self.embeddings[idx_source]\n",
    "       \n",
    "        h = torch.cat((target, neg_samples_target), dim = 0)\n",
    "        t = torch.cat((source, neg_samples_source), dim = 0)\n",
    "        y = torch.cat((pos_labels, neg_labels), dim = 0)\n",
    "        \n",
    "        pred = self.Wr(h,t)\n",
    "        logits = self.sig(pred)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        target, source = batch\n",
    "        pos_labels = torch.zeros((target.shape[0],2), dtype=torch.float)\n",
    "        pos_labels[:,0] = 1\n",
    "        y = pos_labels\n",
    "        pred = self.Wr(target,source)\n",
    "        logits = self.sig(pred)\n",
    "        loss = self.loss(logits, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def classify(self, h, t):\n",
    "        pred = self.Wr(h,t)\n",
    "        logits = self.sig(pred)\n",
    "        return logits[:,0]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_to_r(data, relationship_type):\n",
    "    mask = data.edge_type == relationship_type\n",
    "    edge_index = data.edge_index[:,mask]\n",
    "    return list(zip(embeddings[edge_index[0]], embeddings[edge_index[1]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "topk_acc = torch.tensor(0.0)\n",
    "mmr_acc = torch.tensor(0.0)\n",
    "\n",
    "for rel_type in range(237):\n",
    "    train_set = subset_to_r(train_data, rel_type)\n",
    "    validation_set = subset_to_r(val_data, rel_type)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=1024)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_set, batch_size=1024)\n",
    "    model = Scoring(num_nodes = train_data.num_nodes, batch_size=1024)\n",
    "    trainer = pl.Trainer(max_epochs=200, log_every_n_steps=1, accelerator='cpu', enable_model_summary=False, enable_progress_bar=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(models, r_type):\n",
    "    test_set = subset_to_r(test_data, r_type)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=2048, shuffle=False)\n",
    "    prob_per_r = None\n",
    "    for batch in test_loader:\n",
    "        v_t, v_s = batch\n",
    "        prob_per_r_batch = []\n",
    "        for model in models:\n",
    "            prob_per_r_batch.append(model.classify(v_t, v_s))\n",
    "\n",
    "        if prob_per_r is None:\n",
    "            prob_per_r = torch.stack(prob_per_r_batch).squeeze(1)\n",
    "        else:\n",
    "            prob_per_r = torch.cat((prob_per_r, torch.stack(prob_per_r_batch).squeeze(1)))\n",
    "\n",
    "    return prob_per_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
    "\n",
    "topk_accs = []\n",
    "mmr_accs = []\n",
    "weights = []\n",
    "\n",
    "for rel_type in range(237):\n",
    "    ranking = get_scores(models, rel_type)\n",
    "    if ranking is None:\n",
    "        continue\n",
    "\n",
    "    ranking = ranking.detach().numpy().T\n",
    "\n",
    "    if len(ranking.shape) == 1:\n",
    "        ranking = ranking.reshape(1, -1)\n",
    "\n",
    "    true_labels = np.zeros(ranking.shape[0]) + rel_type\n",
    "\n",
    "    targets = np.zeros((ranking.shape[0], ranking.shape[1]))\n",
    "    targets[:, rel_type] = 1\n",
    "\n",
    "    indices = np.arange(ranking.shape[0]).reshape(-1,1).repeat(ranking.shape[1], axis=1)\n",
    "\n",
    "    topk_acc = RetrievalHitRate(top_k=10)(torch.tensor(ranking), torch.tensor(targets), torch.tensor(indices))\n",
    "    mmr_acc = RetrievalMRR()(torch.tensor(ranking), torch.tensor(targets), torch.tensor(indices))\n",
    "\n",
    "    weights.append(ranking.shape[0])\n",
    "    topk_accs.append(topk_acc)\n",
    "    mmr_accs.append(mmr_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopK Accuracy: 0.7249\n",
      "MMR Accuracy: 0.3661\n"
     ]
    }
   ],
   "source": [
    "# compute weighted average for metrics\n",
    "topk_accs = np.array(topk_accs)\n",
    "mmr_accs = np.array(mmr_accs)\n",
    "weights = np.array(weights)\n",
    "\n",
    "topk_acc = np.average(topk_accs, weights=weights)\n",
    "mmr_acc = np.average(mmr_accs, weights=weights)\n",
    "\n",
    "print(f'TopK Accuracy: {topk_acc:.4f}')\n",
    "print(f'MMR Accuracy: {mmr_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
